{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**1. Business Understanding:**\n",
    "\n",
    "-   **Objective:** Predict the miles per gallon (MPG) of cars based on various attributes.\n",
    "-   **Business Goals:** Improve fuel efficiency predictions for better decision-making regarding car models and environmental impact.\n",
    "\n",
    "**2. Data Understanding:**\n",
    "\n",
    "-   **Data Collection:** Use the Auto MPG dataset, containing information about car attributes and MPG.\n",
    "-   **Exploratory Data Analysis (EDA):**\n",
    "    -   Examine the structure of the dataset.\n",
    "    -   Identify features and their data types.\n",
    "    -   Check for missing values and outliers.\n",
    "    -   Understand the distribution of the target variable (MPG).\n",
    "    -   Explore relationships between features and the target variable.\n",
    "\n",
    "**3. Data Preparation:**\n",
    "\n",
    "-   **Cleaning:**\n",
    "    -   Handle missing values (if any).\n",
    "    -   Convert relevant columns to the correct data types.\n",
    "-   **Feature Engineering:**\n",
    "    -   Create derived attributes or transform existing features.\n",
    "-   **Scaling:**\n",
    "    -   Standardize numerical features using `StandardScaler`.\n",
    "-   **Train-Test Split:**\n",
    "    -   Split the dataset into training and testing sets.\n",
    "\n",
    "**4. Modeling:**\n",
    "\n",
    "-   **Model Selection:**\n",
    "    -   Collect information about multiple possible models, in our case we tested 3 approaches: Linear regression, Neural Network and Gradient Boosting Regressor. After an analysis that will be presented a bit later in this document, we chose Gradient Boosting regressor.\n",
    "    -   Choose the Gradient Boosting Regressor as the modeling technique.\n",
    "-   **Hyperparameter Tuning:**\n",
    "    -   Experiment with hyperparameter values (e.g., learning rate, number of estimators) to optimize model performance.\n",
    "-   **Training:**\n",
    "    -   Train the Gradient Boosting Regressor on the training set.\n",
    "\n",
    "**5. Evaluation:**\n",
    "\n",
    "-   **Model Evaluation:**\n",
    "    -   Evaluate the model's performance on the test set using metrics like Mean Squared Error (MSE).\n",
    "    -   Assess how well the model aligns with business goals.\n",
    "-   **Visualizations:**\n",
    "    -   Create visualizations, such as scatter plots for true vs. predicted values and feature importance plots.\n",
    "\n",
    "**6. Deployment:**\n",
    "\n",
    "-   **Deployment Plan:**\n",
    "    -   Plan the deployment of the trained model into a production environment by wrapping it inside an application built using a traditional REST framework like Django, FLask, etc.\n",
    "-   **Monitoring:**\n",
    "    -   Implement monitoring to track the model's performance in real-world scenarios using Grafana or other tools.\n",
    "-   **Documentation:**\n",
    "    -   Create documentation for end-users and stakeholders: since this project is mostly a back-end application, users won't see it, so the documentation will focus mostly on the devops and support teams, so they can know how to debug and restart the project when needed.\n",
    "\n",
    "**7. Iteration:**\n",
    "\n",
    "-   **Feedback Loop:**\n",
    "    -   Gather feedback from stakeholders.\n",
    "    -   Consider improvements, additional features, or model retraining based on feedback and changing requirements.\n",
    "    -   Iterate through the CRISP-DM process as needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Choosing a model**\n",
    "\n",
    "We will analyse 3 sepparate approaches, each with its unique advantages and drawback. We will implement initially all of them, do a quick test based on the mean squared error, and analyse which one of them is the most accurate for our business case. \n",
    "\n",
    "The 3 models which will be analysed are:\n",
    "\n",
    "- Linear Regression\n",
    "- Neural Network\n",
    "- Gradient Boosting Regressor\n",
    "\n",
    "We will now analyse each one of them:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Before we begin**\n",
    "\n",
    "Before we start working with each model, we will ask you to run the following pieces of code, in order to import the training data, and the required libraries, so we don't import them and trasnform the model again on each iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "import tensorflow as tf\n",
    "# workaround for a Jupyter notebook error when importing tf.keras.models\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "\n",
    "# Load the dataset\n",
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/auto-mpg.data\"\n",
    "column_names = ['mpg', 'cylinders', 'displacement', 'horsepower', 'weight', 'acceleration', 'model_year', 'origin', 'car_name']\n",
    "df = pd.read_csv(url, delim_whitespace=True, names=column_names)\n",
    "\n",
    "# Preprocess the data\n",
    "df.drop('car_name', axis=1, inplace=True)\n",
    "df['horsepower'] = pd.to_numeric(df['horsepower'], errors='coerce')\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# Split the data into features (X) and target variable (y)\n",
    "X = df.drop('mpg', axis=1)\n",
    "y = df['mpg']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**About the data preprocession**\n",
    "\n",
    "to be continued"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1st approach: Linear Regression:**\n",
    "\n",
    "Linear regression is a statistical method used for modeling the relationship between a dependent variable (or target) and one or more independent variables (or features). The goal of linear regression is to find the linear relationship that best fits the data. The simplest form is simple linear regression, which involves a single independent variable, while multiple linear regression deals with multiple independent variables.\n",
    "\n",
    "In a simple linear regression equation, it takes the form:\n",
    "\n",
    "    y=mx+b\n",
    "\n",
    "where:\n",
    "\n",
    "-   y is the dependent variable (target),\n",
    "-   x is the independent variable (feature),\n",
    "-   m is the slope of the line, and\n",
    "-   b is the y-intercept.\n",
    "\n",
    "\n",
    "**Advantages of Linear Regression:**\n",
    "\n",
    "1.  **Interpretability:** Linear regression models are simple and easy to interpret. The coefficients in the equation provide insights into the relationship between variables.\n",
    "    \n",
    "2.  **Computational Efficiency:** Linear regression is computationally efficient, making it suitable for large datasets.\n",
    "    \n",
    "3.  **Quick Implementation:** It is quick to implement and serves as a good baseline model.\n",
    "    \n",
    "4.  **Well-understood:** Linear regression is a well-understood and extensively studied statistical method.\n",
    "    \n",
    "\n",
    "**Drawbacks of Linear Regression:**\n",
    "\n",
    "1.  **Assumption of Linearity:** Linear regression assumes that the relationship between variables is linear. If the true relationship is non-linear, linear regression may provide inaccurate predictions.\n",
    "    \n",
    "2.  **Sensitive to Outliers:** Outliers can significantly influence the model's coefficients and predictions.\n",
    "    \n",
    "3.  **Assumption of Independence:** Linear regression assumes that the errors are independent. Violation of this assumption can affect the model's accuracy.\n",
    "    \n",
    "4.  **Multicollinearity:** When independent variables are highly correlated, it can lead to unstable coefficient estimates.\n",
    "    \n",
    "5.  **Limited Complexity:** Linear regression is not suitable for capturing complex relationships with intricate patterns in the data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Linear regression implementation**\n",
    "\n",
    "We implemented the model using a simple linear regression, and we will test the mean squared error. In the example given above, from multiple runs, we managed to obtain a mean square error of about **10.7**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 10.710864418838396\n",
      "Predicted MPG for the example input: 21.385639953053804\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alex\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Train a linear regression model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f'Mean Squared Error: {mse}')\n",
    "\n",
    "# Example prediction\n",
    "example_input = [[6, 225, 100, 3233, 15.4, 76, 1]]  # Example input data for prediction\n",
    "example_input_scaled = scaler.transform(example_input)\n",
    "predicted_mpg = model.predict(example_input_scaled)\n",
    "print(f'Predicted MPG for the example input: {predicted_mpg[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2nd approach: Neural Networks**\n",
    "\n",
    "Neural networks are a class of machine learning models inspired by the structure and functioning of the human brain. They consist of interconnected nodes (neurons) organized into layers. Neural networks are capable of learning complex patterns and relationships from data through a process called training. The training involves adjusting the weights and biases of the connections between neurons to minimize the difference between predicted and actual outcomes.\n",
    "\n",
    "**Advantages of Neural Networks:**\n",
    "\n",
    "1.  **Non-Linearity:** Neural networks can model complex, non-linear relationships in data, making them suitable for a wide range of tasks.\n",
    "    \n",
    "2.  **Feature Learning:** Neural networks can automatically learn relevant features from the data, reducing the need for manual feature engineering.\n",
    "    \n",
    "3.  **Versatility:** Neural networks are versatile and can be applied to various types of data, including images, text, and sequences.\n",
    "    \n",
    "4.  **Parallel Processing:** The parallel processing capability of neural networks enables them to handle large amounts of data efficiently.\n",
    "    \n",
    "5.  **Adaptability:** Neural networks can adapt and generalize well to different types of problems, making them applicable to a diverse set of tasks.\n",
    "    \n",
    "\n",
    "**Drawbacks of Neural Networks:**\n",
    "\n",
    "1.  **Complexity:** The architecture and hyperparameter tuning of neural networks can be complex and time-consuming.\n",
    "    \n",
    "2.  **Data Requirements:** Neural networks often require large amounts of data to generalize well and avoid overfitting.\n",
    "    \n",
    "3.  **Computationally Intensive:** Training deep neural networks can be computationally intensive and may require specialized hardware.\n",
    "    \n",
    "4.  **Black Box Nature:** The inner workings of complex neural networks can be challenging to interpret, leading to a \"black box\" problem.\n",
    "    \n",
    "5.  **Vulnerability to Noisy Data:** Neural networks may be sensitive to noisy data, outliers, or irrelevant features.\n",
    "    \n",
    "6.  **Hyperparameter Sensitivity:** Neural networks are sensitive to the choice of hyperparameters, and finding the optimal configuration may involve extensive experimentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Neural Networks implementation**\n",
    "\n",
    "Just like we did with the Linear regression, we implemented the model using a neural network, and we will test the mean squared error. \n",
    "In the example given below, from multiple runs, we managed to obtain a mean square error of about **10.2** after 50 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "10/10 - 0s - loss: 592.0648 - val_loss: 532.1111 - 437ms/epoch - 44ms/step\n",
      "Epoch 2/50\n",
      "10/10 - 0s - loss: 566.5259 - val_loss: 505.7727 - 40ms/epoch - 4ms/step\n",
      "Epoch 3/50\n",
      "10/10 - 0s - loss: 538.0164 - val_loss: 474.8748 - 40ms/epoch - 4ms/step\n",
      "Epoch 4/50\n",
      "10/10 - 0s - loss: 502.6318 - val_loss: 438.0248 - 38ms/epoch - 4ms/step\n",
      "Epoch 5/50\n",
      "10/10 - 0s - loss: 461.5247 - val_loss: 393.2238 - 43ms/epoch - 4ms/step\n",
      "Epoch 6/50\n",
      "10/10 - 0s - loss: 411.1219 - val_loss: 341.0549 - 38ms/epoch - 4ms/step\n",
      "Epoch 7/50\n",
      "10/10 - 0s - loss: 351.9496 - val_loss: 282.9438 - 38ms/epoch - 4ms/step\n",
      "Epoch 8/50\n",
      "10/10 - 0s - loss: 288.0170 - val_loss: 221.3045 - 38ms/epoch - 4ms/step\n",
      "Epoch 9/50\n",
      "10/10 - 0s - loss: 221.2415 - val_loss: 161.3786 - 42ms/epoch - 4ms/step\n",
      "Epoch 10/50\n",
      "10/10 - 0s - loss: 157.9914 - val_loss: 108.2342 - 35ms/epoch - 3ms/step\n",
      "Epoch 11/50\n",
      "10/10 - 0s - loss: 105.2013 - val_loss: 68.7048 - 36ms/epoch - 4ms/step\n",
      "Epoch 12/50\n",
      "10/10 - 0s - loss: 66.7083 - val_loss: 47.6189 - 39ms/epoch - 4ms/step\n",
      "Epoch 13/50\n",
      "10/10 - 0s - loss: 47.9209 - val_loss: 40.0513 - 37ms/epoch - 4ms/step\n",
      "Epoch 14/50\n",
      "10/10 - 0s - loss: 40.1764 - val_loss: 37.8862 - 37ms/epoch - 4ms/step\n",
      "Epoch 15/50\n",
      "10/10 - 0s - loss: 36.6546 - val_loss: 35.4631 - 35ms/epoch - 4ms/step\n",
      "Epoch 16/50\n",
      "10/10 - 0s - loss: 33.4129 - val_loss: 32.4503 - 35ms/epoch - 4ms/step\n",
      "Epoch 17/50\n",
      "10/10 - 0s - loss: 30.5659 - val_loss: 29.8349 - 36ms/epoch - 4ms/step\n",
      "Epoch 18/50\n",
      "10/10 - 0s - loss: 28.0059 - val_loss: 27.6144 - 33ms/epoch - 3ms/step\n",
      "Epoch 19/50\n",
      "10/10 - 0s - loss: 26.0078 - val_loss: 25.6797 - 33ms/epoch - 3ms/step\n",
      "Epoch 20/50\n",
      "10/10 - 0s - loss: 24.2007 - val_loss: 24.0581 - 36ms/epoch - 4ms/step\n",
      "Epoch 21/50\n",
      "10/10 - 0s - loss: 22.6795 - val_loss: 22.5035 - 35ms/epoch - 3ms/step\n",
      "Epoch 22/50\n",
      "10/10 - 0s - loss: 21.3093 - val_loss: 21.3260 - 35ms/epoch - 4ms/step\n",
      "Epoch 23/50\n",
      "10/10 - 0s - loss: 20.2650 - val_loss: 20.1513 - 35ms/epoch - 3ms/step\n",
      "Epoch 24/50\n",
      "10/10 - 0s - loss: 19.2294 - val_loss: 19.1164 - 35ms/epoch - 3ms/step\n",
      "Epoch 25/50\n",
      "10/10 - 0s - loss: 18.3146 - val_loss: 18.2277 - 32ms/epoch - 3ms/step\n",
      "Epoch 26/50\n",
      "10/10 - 0s - loss: 17.6600 - val_loss: 17.5417 - 31ms/epoch - 3ms/step\n",
      "Epoch 27/50\n",
      "10/10 - 0s - loss: 16.8433 - val_loss: 16.7027 - 32ms/epoch - 3ms/step\n",
      "Epoch 28/50\n",
      "10/10 - 0s - loss: 16.2318 - val_loss: 15.9310 - 34ms/epoch - 3ms/step\n",
      "Epoch 29/50\n",
      "10/10 - 0s - loss: 15.6324 - val_loss: 15.3511 - 33ms/epoch - 3ms/step\n",
      "Epoch 30/50\n",
      "10/10 - 0s - loss: 15.1829 - val_loss: 14.8122 - 34ms/epoch - 3ms/step\n",
      "Epoch 31/50\n",
      "10/10 - 0s - loss: 14.6455 - val_loss: 14.4079 - 33ms/epoch - 3ms/step\n",
      "Epoch 32/50\n",
      "10/10 - 0s - loss: 14.2709 - val_loss: 13.9854 - 31ms/epoch - 3ms/step\n",
      "Epoch 33/50\n",
      "10/10 - 0s - loss: 13.9394 - val_loss: 13.5324 - 38ms/epoch - 4ms/step\n",
      "Epoch 34/50\n",
      "10/10 - 0s - loss: 13.5759 - val_loss: 13.0387 - 35ms/epoch - 3ms/step\n",
      "Epoch 35/50\n",
      "10/10 - 0s - loss: 13.2639 - val_loss: 12.6539 - 35ms/epoch - 3ms/step\n",
      "Epoch 36/50\n",
      "10/10 - 0s - loss: 12.9623 - val_loss: 12.3969 - 33ms/epoch - 3ms/step\n",
      "Epoch 37/50\n",
      "10/10 - 0s - loss: 12.7014 - val_loss: 12.1150 - 34ms/epoch - 3ms/step\n",
      "Epoch 38/50\n",
      "10/10 - 0s - loss: 12.4627 - val_loss: 11.8132 - 32ms/epoch - 3ms/step\n",
      "Epoch 39/50\n",
      "10/10 - 0s - loss: 12.2431 - val_loss: 11.4842 - 33ms/epoch - 3ms/step\n",
      "Epoch 40/50\n",
      "10/10 - 0s - loss: 12.0368 - val_loss: 11.2420 - 33ms/epoch - 3ms/step\n",
      "Epoch 41/50\n",
      "10/10 - 0s - loss: 11.8412 - val_loss: 11.0875 - 33ms/epoch - 3ms/step\n",
      "Epoch 42/50\n",
      "10/10 - 0s - loss: 11.6461 - val_loss: 10.9610 - 39ms/epoch - 4ms/step\n",
      "Epoch 43/50\n",
      "10/10 - 0s - loss: 11.4480 - val_loss: 10.6880 - 40ms/epoch - 4ms/step\n",
      "Epoch 44/50\n",
      "10/10 - 0s - loss: 11.2843 - val_loss: 10.4243 - 32ms/epoch - 3ms/step\n",
      "Epoch 45/50\n",
      "10/10 - 0s - loss: 11.1803 - val_loss: 10.2445 - 34ms/epoch - 3ms/step\n",
      "Epoch 46/50\n",
      "10/10 - 0s - loss: 10.9923 - val_loss: 10.1758 - 32ms/epoch - 3ms/step\n",
      "Epoch 47/50\n",
      "10/10 - 0s - loss: 10.8770 - val_loss: 9.9945 - 33ms/epoch - 3ms/step\n",
      "Epoch 48/50\n",
      "10/10 - 0s - loss: 10.7367 - val_loss: 9.9369 - 32ms/epoch - 3ms/step\n",
      "Epoch 49/50\n",
      "10/10 - 0s - loss: 10.6171 - val_loss: 9.7044 - 39ms/epoch - 4ms/step\n",
      "Epoch 50/50\n",
      "10/10 - 0s - loss: 10.4859 - val_loss: 9.5626 - 38ms/epoch - 4ms/step\n",
      "Mean Squared Error: 9.56257588087463\n",
      "Predicted MPG for the example input: 15.406335830688477\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alex\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Build the neural network model\n",
    "model = Sequential()\n",
    "model.add(Dense(64, input_dim=X_train_scaled.shape[1], activation='relu'))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(1, activation='linear'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train_scaled, y_train, epochs=50, batch_size=32, validation_data=(X_test_scaled, y_test), verbose=2)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f'Mean Squared Error: {mse}')\n",
    "\n",
    "# Example prediction\n",
    "example_input = np.array([[6, 225, 100, 3233, 15.4, 76, 1]])  # Example input data for prediction\n",
    "example_input_scaled = scaler.transform(example_input)\n",
    "predicted_mpg = model.predict(example_input_scaled)\n",
    "print(f'Predicted MPG for the example input: {predicted_mpg[0][0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3rd approach: Gradient Boosting Regressor**\n",
    "\n",
    "\n",
    "Gradient Boosting is an ensemble learning technique that builds a series of weak learners, typically decision trees, and combines their predictions to create a strong predictive model. Gradient Boosting Regressor specifically focuses on regression tasks, aiming to predict a continuous target variable.\n",
    "\n",
    "**Advantages of Gradient Boosting Regressor:**\n",
    "\n",
    "1.  **High Predictive Accuracy:** Gradient Boosting Regressor often achieves high predictive accuracy and outperforms many other algorithms.\n",
    "    \n",
    "2.  **Handles Non-Linearity and Interactions:** The ensemble nature of gradient boosting allows it to capture non-linear relationships and interactions between features.\n",
    "    \n",
    "3.  **Robustness to Outliers:** Gradient Boosting is less sensitive to outliers in the dataset compared to some other models.\n",
    "    \n",
    "4.  **Feature Importance:** Provides a feature importance ranking, helping to identify the most influential variables in making predictions.\n",
    "    \n",
    "5.  **Flexible with Different Loss Functions:** Can be used with various loss functions, making it adaptable to different types of regression problems.\n",
    "    \n",
    "\n",
    "**Drawbacks of Gradient Boosting Regressor:**\n",
    "\n",
    "1.  **Computationally Intensive:** Training a large number of trees in the ensemble can be computationally intensive, especially for deep trees.\n",
    "    \n",
    "2.  **Potential for Overfitting:** Gradient Boosting Regressor can be prone to overfitting, especially if the model is too complex or the learning rate is too high.\n",
    "    \n",
    "3.  **Need for Careful Hyperparameter Tuning:** Effective use of Gradient Boosting Regressor often requires careful tuning of hyperparameters, such as the number of trees and learning rate.\n",
    "    \n",
    "4.  **Less Interpretability:** While it provides feature importance, the interpretability of the model is lower compared to simpler models like linear regression.\n",
    "    \n",
    "5.  **Sensitive to Noisy Data:** Gradient Boosting Regressor can be sensitive to noisy data and outliers, impacting its performance.\n",
    "    \n",
    "6.  **Potential for Bias:** If the training dataset is imbalanced, the model may be biased toward the majority class.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Gradient Boosting Regressor implementation**\n",
    "\n",
    "Just like we did with the Linear regression and the NEural Neteorks, we implemented the model using a Gradient Boosting Regressor, and we will test the mean squared error. \n",
    "In the example given below, from multiple runs, we managed to obtain a mean square error of about **6.2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error (Gradient Boosting): 6.232502768358345\n",
      "Predicted MPG for the example input (Gradient Boosting): 19.23407630490937\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alex\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Build and train the Gradient Boosting Regressor model\n",
    "model_gb = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, random_state=42)\n",
    "model_gb.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred_gb = model_gb.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate the model\n",
    "mse_gb = mean_squared_error(y_test, y_pred_gb)\n",
    "print(f'Mean Squared Error (Gradient Boosting): {mse_gb}')\n",
    "\n",
    "# Example prediction\n",
    "example_input_gb = [[6, 225, 100, 3233, 15.4, 76, 1]]  # Example input data for prediction\n",
    "example_input_gb_scaled = scaler.transform(example_input_gb)\n",
    "predicted_mpg_gb = model_gb.predict(example_input_gb_scaled)\n",
    "print(f'Predicted MPG for the example input (Gradient Boosting): {predicted_mpg_gb[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Error analysis and model selection**\n",
    "\n",
    "\n",
    "\n",
    "Following the experiments ran until now, we can conclude that for our business case, the Gradient Boosting Regressor is the best choice, since it gave the smallest error, and it also ran in a pretty decent time.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
